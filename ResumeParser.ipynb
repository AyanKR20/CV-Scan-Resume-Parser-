{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2508632,"sourceType":"datasetVersion","datasetId":1519260},{"sourceId":8246856,"sourceType":"datasetVersion","datasetId":4892782}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install openai\n!pip install pinecone-client\n!pip install langchain\n!pip install unstructured\n!pip install unstructured[pdf]\n!pip install -U langchain-openai\n!pip install chromadb\n!pip install pypdf\n!pip install tqdm\n!pip install sentence-transformers\n!pip install transformers\n!pip install InstructorEmbedding\n!pip install bitsandbytes\n!pip install accelerate\n!pip install xformers\n!pip install einops\n!apt-get install -y poppler-utils","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers datasets spacy scikit-learn\n!python -m spacy download en_core_web_sm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install pyngrok\n! pip install flask-ngrok\n! ngrok authtoken 2fgafRv4DMXaDwep9sSlHpsPgeB_76vC81WKH5ns7owcmDeWC\n! pip install streamlit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%writefile documents.py\nimport os\nfrom langchain_community.document_loaders import UnstructuredPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom tqdm import tqdm\nimport numpy as np\n\nlist_of_fields=[x[0] for x in os.walk(\"/kaggle/input/resume-dataset/data/data/\")]\nlist_of_fields.pop(0)\nfor i in range(len(list_of_fields)):\n    list_of_fields[i]=list_of_fields[i].replace(\"/kaggle/input/resume-dataset/data/data/\",\"\")\n    \n#error in 14\ndirectory = '/kaggle/input/resume-dataset/data/data/'\ndocs=os.listdir(directory)\ndocuments=[]\nprint(list_of_fields[-4])\nfor field in tqdm([list_of_fields[-4]]):\n    field_directory=directory+field\n    docs=os.listdir(field_directory)\n    for doc in tqdm(docs):\n        loader = UnstructuredPDFLoader(os.path.join(field_directory,doc))\n        data = loader.load()\n        documents.append(data[0])\n        \ndef split_docs(documents, chunk_size=4000, chunk_overlap=100):\n  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n  docs = text_splitter.split_documents(documents)\n  return docs\ndocs=split_docs(documents)\nnp.save(\"docs_array\",docs)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T13:27:07.463359Z","iopub.execute_input":"2024-04-28T13:27:07.464183Z","iopub.status.idle":"2024-04-28T13:29:02.382898Z","shell.execute_reply.started":"2024-04-28T13:27:07.464146Z","shell.execute_reply":"2024-04-28T13:29:02.381955Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"ENGINEERING\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]\n  0%|          | 0/118 [00:00<?, ?it/s]\u001b[A\n  1%|          | 1/118 [00:00<01:33,  1.25it/s]\u001b[A\n  2%|▏         | 2/118 [00:01<01:13,  1.59it/s]\u001b[A\n  3%|▎         | 3/118 [00:02<01:26,  1.33it/s]\u001b[A\n  3%|▎         | 4/118 [00:03<01:41,  1.12it/s]\u001b[A\n  4%|▍         | 5/118 [00:04<01:35,  1.18it/s]\u001b[A\n  5%|▌         | 6/118 [00:04<01:30,  1.24it/s]\u001b[A\n  6%|▌         | 7/118 [00:05<01:32,  1.21it/s]\u001b[A\n  7%|▋         | 8/118 [00:06<01:22,  1.33it/s]\u001b[A\n  8%|▊         | 9/118 [00:07<01:22,  1.32it/s]\u001b[A\n  8%|▊         | 10/118 [00:08<02:01,  1.13s/it]\u001b[A\n  9%|▉         | 11/118 [00:10<02:07,  1.19s/it]\u001b[A\n 10%|█         | 12/118 [00:11<01:52,  1.06s/it]\u001b[A\n 11%|█         | 13/118 [00:11<01:38,  1.07it/s]\u001b[A\n 12%|█▏        | 14/118 [00:12<01:39,  1.04it/s]\u001b[A\n 13%|█▎        | 15/118 [00:13<01:32,  1.12it/s]\u001b[A\n 14%|█▎        | 16/118 [00:14<01:35,  1.07it/s]\u001b[A\n 14%|█▍        | 17/118 [00:14<01:13,  1.38it/s]\u001b[A\n 15%|█▌        | 18/118 [00:15<01:17,  1.29it/s]\u001b[A\n 16%|█▌        | 19/118 [00:16<01:18,  1.25it/s]\u001b[A\n 17%|█▋        | 20/118 [00:16<01:04,  1.52it/s]\u001b[A\n 18%|█▊        | 21/118 [00:17<01:11,  1.36it/s]\u001b[A\n 19%|█▊        | 22/118 [00:19<01:30,  1.06it/s]\u001b[A\n 19%|█▉        | 23/118 [00:19<01:24,  1.12it/s]\u001b[A\n 20%|██        | 24/118 [00:21<01:28,  1.06it/s]\u001b[A\n 21%|██        | 25/118 [00:21<01:13,  1.27it/s]\u001b[A\n 22%|██▏       | 26/118 [00:22<01:11,  1.28it/s]\u001b[A\n 23%|██▎       | 27/118 [00:23<01:14,  1.22it/s]\u001b[A\n 24%|██▎       | 28/118 [00:24<01:21,  1.11it/s]\u001b[A\n 25%|██▍       | 29/118 [00:24<01:03,  1.40it/s]\u001b[A\n 25%|██▌       | 30/118 [00:25<01:21,  1.09it/s]\u001b[A\n 26%|██▋       | 31/118 [00:26<01:23,  1.04it/s]\u001b[A\n 27%|██▋       | 32/118 [00:27<01:18,  1.10it/s]\u001b[A\n 28%|██▊       | 33/118 [00:29<01:27,  1.03s/it]\u001b[A\n 29%|██▉       | 34/118 [00:29<01:20,  1.05it/s]\u001b[A\n 30%|██▉       | 35/118 [00:30<01:07,  1.23it/s]\u001b[A\n 31%|███       | 36/118 [00:31<01:14,  1.10it/s]\u001b[A\n 31%|███▏      | 37/118 [00:32<01:08,  1.19it/s]\u001b[A\n 32%|███▏      | 38/118 [00:32<01:06,  1.20it/s]\u001b[A\n 33%|███▎      | 39/118 [00:33<01:02,  1.26it/s]\u001b[A\n 34%|███▍      | 40/118 [00:34<00:59,  1.32it/s]\u001b[A\n 35%|███▍      | 41/118 [00:35<01:05,  1.18it/s]\u001b[A\n 36%|███▌      | 42/118 [00:36<01:08,  1.11it/s]\u001b[A\n 36%|███▋      | 43/118 [00:37<01:05,  1.14it/s]\u001b[A\n 37%|███▋      | 44/118 [00:38<01:09,  1.07it/s]\u001b[A\n 38%|███▊      | 45/118 [00:39<01:16,  1.05s/it]\u001b[A\n 39%|███▉      | 46/118 [00:40<01:01,  1.16it/s]\u001b[A\n 40%|███▉      | 47/118 [00:40<01:00,  1.17it/s]\u001b[A\n 41%|████      | 48/118 [00:41<00:58,  1.19it/s]\u001b[A\n 42%|████▏     | 49/118 [00:42<01:04,  1.07it/s]\u001b[A\n 42%|████▏     | 50/118 [00:43<01:04,  1.06it/s]\u001b[A\n 43%|████▎     | 51/118 [00:45<01:12,  1.08s/it]\u001b[A\n 44%|████▍     | 52/118 [00:46<01:07,  1.03s/it]\u001b[A\n 45%|████▍     | 53/118 [00:46<00:53,  1.22it/s]\u001b[A\n 46%|████▌     | 54/118 [00:47<00:57,  1.11it/s]\u001b[A\n 47%|████▋     | 55/118 [00:48<00:57,  1.09it/s]\u001b[A\n 47%|████▋     | 56/118 [00:49<00:54,  1.13it/s]\u001b[A\n 48%|████▊     | 57/118 [00:50<01:01,  1.01s/it]\u001b[A\n 49%|████▉     | 58/118 [00:51<01:03,  1.06s/it]\u001b[A\n 50%|█████     | 59/118 [00:52<00:56,  1.04it/s]\u001b[A\n 51%|█████     | 60/118 [00:53<00:53,  1.08it/s]\u001b[A\n 52%|█████▏    | 61/118 [00:54<00:50,  1.12it/s]\u001b[A\n 53%|█████▎    | 62/118 [00:55<00:55,  1.00it/s]\u001b[A\n 53%|█████▎    | 63/118 [00:55<00:46,  1.19it/s]\u001b[A\n 54%|█████▍    | 64/118 [00:56<00:43,  1.25it/s]\u001b[A\n 55%|█████▌    | 65/118 [00:57<00:42,  1.24it/s]\u001b[A\n 56%|█████▌    | 66/118 [00:58<00:43,  1.19it/s]\u001b[A\n 57%|█████▋    | 67/118 [00:58<00:37,  1.35it/s]\u001b[A\n 58%|█████▊    | 68/118 [01:00<00:48,  1.03it/s]\u001b[A\n 58%|█████▊    | 69/118 [01:01<00:55,  1.13s/it]\u001b[A\n 59%|█████▉    | 70/118 [01:03<00:55,  1.16s/it]\u001b[A\n 60%|██████    | 71/118 [01:04<00:54,  1.15s/it]\u001b[A\n 61%|██████    | 72/118 [01:05<00:57,  1.25s/it]\u001b[A\n 62%|██████▏   | 73/118 [01:06<00:53,  1.19s/it]\u001b[A\n 63%|██████▎   | 74/118 [01:07<00:48,  1.09s/it]\u001b[A\n 64%|██████▎   | 75/118 [01:10<01:11,  1.67s/it]\u001b[A\n 64%|██████▍   | 76/118 [01:11<00:59,  1.43s/it]\u001b[A\n 65%|██████▌   | 77/118 [01:11<00:46,  1.12s/it]\u001b[A\n 66%|██████▌   | 78/118 [01:12<00:43,  1.08s/it]\u001b[A\n 67%|██████▋   | 79/118 [01:13<00:38,  1.02it/s]\u001b[A\n 68%|██████▊   | 80/118 [01:15<00:43,  1.16s/it]\u001b[A\n 69%|██████▊   | 81/118 [01:16<00:42,  1.14s/it]\u001b[A\n 69%|██████▉   | 82/118 [01:17<00:40,  1.14s/it]\u001b[A\n 70%|███████   | 83/118 [01:18<00:39,  1.12s/it]\u001b[A\n 71%|███████   | 84/118 [01:19<00:40,  1.20s/it]\u001b[A\n 72%|███████▏  | 85/118 [01:20<00:35,  1.09s/it]\u001b[A\n 73%|███████▎  | 86/118 [01:21<00:33,  1.05s/it]\u001b[A\n 74%|███████▎  | 87/118 [01:22<00:26,  1.15it/s]\u001b[A\n 75%|███████▍  | 88/118 [01:22<00:25,  1.18it/s]\u001b[A\n 75%|███████▌  | 89/118 [01:23<00:24,  1.20it/s]\u001b[A\n 76%|███████▋  | 90/118 [01:24<00:23,  1.20it/s]\u001b[A\n 77%|███████▋  | 91/118 [01:25<00:26,  1.03it/s]\u001b[A\n 78%|███████▊  | 92/118 [01:28<00:40,  1.55s/it]\u001b[A\n 79%|███████▉  | 93/118 [01:29<00:34,  1.39s/it]\u001b[A\n 80%|███████▉  | 94/118 [01:31<00:34,  1.43s/it]\u001b[A\n 81%|████████  | 95/118 [01:31<00:27,  1.19s/it]\u001b[A\n 81%|████████▏ | 96/118 [01:34<00:32,  1.48s/it]\u001b[A\n 82%|████████▏ | 97/118 [01:35<00:27,  1.32s/it]\u001b[A\n 83%|████████▎ | 98/118 [01:35<00:23,  1.16s/it]\u001b[A\n 84%|████████▍ | 99/118 [01:36<00:21,  1.11s/it]\u001b[A\n 85%|████████▍ | 100/118 [01:37<00:17,  1.01it/s]\u001b[A\n 86%|████████▌ | 101/118 [01:38<00:15,  1.09it/s]\u001b[A\n 86%|████████▋ | 102/118 [01:39<00:14,  1.10it/s]\u001b[A\n 87%|████████▋ | 103/118 [01:39<00:11,  1.25it/s]\u001b[A\n 88%|████████▊ | 104/118 [01:40<00:10,  1.30it/s]\u001b[A\n 89%|████████▉ | 105/118 [01:41<00:12,  1.06it/s]\u001b[A\n 90%|████████▉ | 106/118 [01:42<00:11,  1.05it/s]\u001b[A\n 91%|█████████ | 107/118 [01:43<00:10,  1.06it/s]\u001b[A\n 92%|█████████▏| 108/118 [01:44<00:08,  1.14it/s]\u001b[A\n 92%|█████████▏| 109/118 [01:45<00:07,  1.22it/s]\u001b[A\n 93%|█████████▎| 110/118 [01:46<00:07,  1.06it/s]\u001b[A\n 94%|█████████▍| 111/118 [01:47<00:06,  1.08it/s]\u001b[A\n 95%|█████████▍| 112/118 [01:47<00:05,  1.11it/s]\u001b[A\n 96%|█████████▌| 113/118 [01:49<00:04,  1.06it/s]\u001b[A\n 97%|█████████▋| 114/118 [01:49<00:03,  1.23it/s]\u001b[A\n 97%|█████████▋| 115/118 [01:51<00:03,  1.30s/it]\u001b[A\n 98%|█████████▊| 116/118 [01:52<00:02,  1.13s/it]\u001b[A\n 99%|█████████▉| 117/118 [01:54<00:01,  1.29s/it]\u001b[A\n100%|██████████| 118/118 [01:54<00:00,  1.03it/s]\u001b[A\n100%|██████████| 1/1 [01:54<00:00, 114.82s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile retriever.py\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.vectorstores import Chroma\nimport numpy as np\ndocs=list(np.load(\"docs_array.npy\",allow_pickle=True))\n\npersist_directory = 'db'\nembeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\nvectordb = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory=persist_directory)\n\n# persiste the db to disk\nvectordb.persist()\nvectordb = None\n\n# Now we can load the persisted database from disk, and use it as normal.\nvectordb = Chroma(persist_directory=persist_directory,embedding_function=embeddings)\n\nretriever = vectordb.as_retriever(search_kwargs={\"k\": 5}) # by default search_type=\"similarity_score_threshold\"","metadata":{"execution":{"iopub.status.busy":"2024-04-28T16:03:17.347182Z","iopub.execute_input":"2024-04-28T16:03:17.347991Z","iopub.status.idle":"2024-04-28T16:03:17.354748Z","shell.execute_reply.started":"2024-04-28T16:03:17.347955Z","shell.execute_reply":"2024-04-28T16:03:17.353631Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stdout","text":"Overwriting retriever.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"LLM Implementation","metadata":{}},{"cell_type":"code","source":"%%writefile chatter.py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.chains import ConversationChain\nfrom langchain.chains.conversation.memory import ConversationSummaryMemory\n\nMODEL_NAME = \"tiiuae/falcon-7b-instruct\"\nTOKEN = \"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\"\n\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME, \n                                             trust_remote_code=True, \n                                             load_in_8bit=True, \n                                             device_map=\"auto\", \n                                             token = TOKEN\n)\n\nmodel = model.eval()\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ngeneration_pipeline = pipeline(\n            model=model,\n            tokenizer=tokenizer,\n            return_full_text=False,\n            task=\"text-generation\",\n        )\n\nllm = HuggingFacePipeline(pipeline=generation_pipeline)\n\ngeneration_config = model.generation_config\ngeneration_config.temperature = 0\ngeneration_config.num_return_sequences = 1\ngeneration_config.max_new_tokens = 256\ngeneration_config.use_cache = False\ngeneration_config.repetition_penalty = 1.7\ngeneration_config.pad_token_id = tokenizer.eos_token_id\ngeneration_config.eos_token_id = tokenizer.eos_token_id\ngeneration_config\n\nmemory = ConversationSummaryMemory(llm=llm)\n\nconversation_chain = ConversationChain(\n    llm=llm,\n    memory=memory,\n    verbose = True\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-28T16:03:10.111607Z","iopub.execute_input":"2024-04-28T16:03:10.112440Z","iopub.status.idle":"2024-04-28T16:03:10.121511Z","shell.execute_reply.started":"2024-04-28T16:03:10.112399Z","shell.execute_reply":"2024-04-28T16:03:10.120451Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":"Overwriting chatter.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile jd_comp.py\nimport spacy\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport json\n\n\n# Initialize SpaCy for NLP tasks\nnlp = spacy.load('en_core_web_sm')\n\n# Function to preprocess text documents\ndef preprocess_text(texts):\n    if isinstance(texts, list):\n        texts = \" \".join(texts)\n    doc = nlp(texts)\n    keywords = [token.lemma_.lower() for token in doc if token.pos_ in {'NOUN', 'VERB'} and not token.is_stop]\n    return \" \".join(keywords)\n# Function to compute semantic similarity using Transformers\ndef get_similarity(text1, text2, model_name='sentence-transformers/all-mpnet-base-v2'):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModel.from_pretrained(model_name)\n    # Encode the texts\n    inputs = tokenizer([text1, text2], return_tensors='pt', padding=True, truncation=True, max_length=512)\n    with torch.no_grad():\n        model_output = model(**inputs)\n        embeddings = model_output.last_hidden_state[:, 0, :]  # CLS token embeddings\n    # Compute cosine similarity\n    cosine_similarity = torch.nn.functional.cosine_similarity(embeddings[0].unsqueeze(0), embeddings[1].unsqueeze(0)).item()\n    return cosine_similarity\n\ndef generate_suggestions(similarity_score):\n    suggestions = []\n    # Example threshold and suggestions, adjust based on your analysis and requirements\n    if similarity_score < 0.5:  # Assuming a similarity score range of 0 to 1\n        suggestions.append(\"Consider highlighting relevant experiences that match the job description.\")\n        suggestions.append(\"Make sure your CV mentions key skills listed in the job description.\")\n        # Additional logic to generate more specific suggestions can be added here\n    elif similarity_score >= 0.5 and similarity_score < 0.75:\n        suggestions.append(\"Your CV is on the right track, but further aligning it with the job description could improve your chances.\")\n    else:\n        suggestions.append(\"Your CV closely matches the job description. Ensure it's tailored and precise for the best impact.\")\n\n    return suggestions","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:00:25.693147Z","iopub.execute_input":"2024-04-28T17:00:25.694592Z","iopub.status.idle":"2024-04-28T17:00:25.702609Z","shell.execute_reply.started":"2024-04-28T17:00:25.694553Z","shell.execute_reply":"2024-04-28T17:00:25.701559Z"},"trusted":true},"execution_count":134,"outputs":[{"name":"stdout","text":"Overwriting jd_comp.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile my_app.py\nimport streamlit as st\nfrom langchain_community.document_loaders import UnstructuredPDFLoader\nfrom retriever import retriever\nfrom chatter import conversation_chain, memory\nfrom jd_comp import preprocess_text, get_similarity, generate_suggestions\nmemory.clear()\n\nst.title(\"How Can We Help You In Improving Your Resume?\")\nst.config.set_option(\"server.maxUploadSize\", 2)\n\noption = st.selectbox(\"\",\n    (\"Analyse Resume\", \"Compare Resume With Job Description\")\n)\nupload_check=False\nif option == \"Analyse Resume\":\n    resume_file = st.file_uploader(\"Please Upload Your Resume\",type=[\"pdf\"])\n    if resume_file is not None:\n        loader=None\n        with open('resume_file.pdf', 'wb') as f:\n            f.write(resume_file.getvalue())\n        loader = UnstructuredPDFLoader(\"/kaggle/working/resume_file.pdf\")\n        data = loader.load()\n        resume_data=data[0].page_content\n        sub_field = st.text_input(\"Which sub-field do you want your resume to fit to?\", value=None)\n        if sub_field is not None:\n            query = \"which resume is the best for \" + sub_field\n            matched_resumes=retriever.invoke(query)\n            top=2\n            prompt = \"\"\n            best_resumes=\"\"\n            for i in range(top):\n                best_resumes+=matched_resumes[i].page_content\n            prompt = f\"\"\"You have to suggest improvements for a resume. Here are contents of the good resumes:\\n\\n\"{best_resumes}\"\\n\\nHere is the resume to be improved:\\n\\n\"{resume_data}\"\\n\\nDo not make the response too long\"\"\".strip()\n            st.write(\"Suggesting Improvements:\")\n            response = conversation_chain.predict(input = prompt)\n            st.write(response)\n\n            \nelif option == \"Compare Resume With Job Description\":\n    resume_file = st.file_uploader(\"Please Upload Your Resume\",type=[\"pdf\"])\n    jd_file = st.file_uploader(\"Please Upload The Job Description\",type=[\"pdf\"])\n    if resume_file is not None and jd_file is not None:\n        with open('resume_file.pdf', 'wb') as f:\n            f.write(resume_file.getvalue())\n        with open('jd_file.pdf', 'wb') as f:\n            f.write(jd_file.getvalue())\n            \n        resume_loader = UnstructuredPDFLoader(\"/kaggle/working/resume_file.pdf\")\n        cv_data = resume_loader.load()\n        cv_texts = [doc.page_content for doc in cv_data]  # Adjust based on the actual structure\n        concatenated_text_cv = \" \".join(cv_texts)\n\n        jd_loader = UnstructuredPDFLoader(\"/kaggle/working/jd_file.pdf\")  # Replace with the actual file path to the job description\n        jd_data = jd_loader.load()\n        jd_texts = [doc.page_content for doc in jd_data]\n        concatenated_text_jd = \" \".join(jd_texts)\n        \n        cv_processed = preprocess_text(concatenated_text_cv)\n        job_description_processed = preprocess_text(concatenated_text_jd)\n        print(cv_processed)\n        print(job_description_processed)\n        similarity_score = get_similarity(cv_processed, job_description_processed)\n        suggestions = generate_suggestions(similarity_score)\n        suggestions = \" \".join(suggestions)\n        st.write(\"Suggestions for improving your CV:\")\n        st.write(suggestions)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:00:30.305835Z","iopub.execute_input":"2024-04-28T17:00:30.306576Z","iopub.status.idle":"2024-04-28T17:00:30.314613Z","shell.execute_reply.started":"2024-04-28T17:00:30.306541Z","shell.execute_reply":"2024-04-28T17:00:30.313432Z"},"trusted":true},"execution_count":135,"outputs":[{"name":"stdout","text":"Overwriting my_app.py\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyngrok import ngrok\nngrok.set_auth_token(\"2fgafRv4DMXaDwep9sSlHpsPgeB_76vC81WKH5ns7owcmDeWC\")\n\npublic_url =  ngrok.connect(80).public_url\nprint(f\"Please click on the text below {public_url}\")\n!streamlit run --server.port 80 my_app.py > /dev/null\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T17:03:24.640011Z","iopub.execute_input":"2024-04-28T17:03:24.641062Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Please click on the text below https://553b-104-154-185-96.ngrok-free.app\n/opt/conda/lib/python3.10/site-packages/langchain/embeddings/__init__.py:29: LangChainDeprecationWarning: Importing embeddings from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.embeddings import SentenceTransformerEmbeddings`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/langchain/vectorstores/__init__.py:35: LangChainDeprecationWarning: Importing vector stores from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.vectorstores import Chroma`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\n2024-04-28 17:03:50.160363: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-28 17:03:50.160427: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-28 17:03:50.162149: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/langchain/llms/__init__.py:548: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.llms import HuggingFacePipeline`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\nThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:13<00:00,  6.88s/it]\n2024-04-28 17:04:10.663 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n2024-04-28 17:04:31.480 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n2024-04-28 17:04:51.282 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nToken indices sequence length is longer than the specified maximum sequence length for this model (2109 > 2048). Running this sequence through the model will result in indexing errors\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n2024-04-28 17:18:53.456 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"}]}]}